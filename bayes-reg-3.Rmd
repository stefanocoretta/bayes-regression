---
title: "Introduction to Bayesian linear regression with brms --- Part III"
author: "Stefano Coretta"
date: "26/06/2020"
output: 
  pdf_document: 
    highlight: tango
    keep_tex: yes
    latex_engine: xelatex
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = here::here())
library(tidyverse)
theme_set(theme_minimal())
library(patchwork)
library(brms)
options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE)
library(bayesplot)
library(extraDistr)
```

# Quick recap

We collect a sample of observations $y = (y_1, ..., y_n)$ generated by a **random variable** $Y$, i.e. we sample values generated by a random event.

We use the sample $y$ to obtain an estimate of $Y$, i.e. we perform statistical inference.

$Y$ is both *unknown* (we don't know the value it takes) and *uncertain* (we can't be sure about the value it takes). Uncertainty can be expressed by probability distributions.

A **probability distribution** is a list of values along with its corresponding probability. Probabilities take on values between 0 (impossible event) and 1 (certain event).

Probability distributions can be summarised using **parameters**. The *normal distribution* (aka Gaussian distribution) has two parameters: the mean $\mu$ and the standard deviation $\sigma$ (or the variance $\sigma^2$). Parameters are random variables themselves, i.e. they are uncertain. We can express distribution parameters with probability distributions (which have *hyperparameters*, or in other words parameters on parameters). We can estimate the hyperparameters from the sample $y$.

For example:

$$ y_i \sim Normal(\mu, \sigma) $$
$$ \mu \sim Normal(\mu_1, \sigma_1)$$
$$ \sigma \sim HalfCauchy(x_0, \gamma)$$

$y$ is our outcome variable (our sample of values).
We believe that the values of $y$ were generated by a normal distribution which has parameters $\mu$ and $\sigma$ (this is the *likelihood*, or simply the *probability distribution of the outcome variable*).
We believe that the value of $\mu$ comes from a normal distribution with $\mu_1$ and $\sigma_1$ and that the value of $\sigma$ comes from a half Cauchy distribution with parameters $x_0$ and $\gamma$.

We convey our prior beliefs of what $\mu$ and $\sigma$ might be by specifying probability distributions (the **prior probability distributions**, or *priors*).
In other words, we specify the hyperparameters $\mu_1$, $\sigma_1$, $x_0$, $\gamma$.

These hyperparameters are updated by the Bayesian model with the information provided by the sample, and we thus obtain the **posterior probability distributions** (or *posteriors*) of $\mu$ and $\sigma$.

# Discrete variables

In Part I and II we focussed on *continous variables* and continous probability distributions, among which the normal (Gaussian) distribution is the most commonly employed for statistical analyses.
A continous variable is a variable that can take on any value between two specific values (numbers).
For example, height, weight, vowel formants, segment duration, and reaction times are all continous variables.

Some events, on the other hand, generate **discrete variables**, i.e. variables that can only take specific values and not values in between.
For example, flips of a coin, counts, yes/no questions, success/failure are all discrete variables (if you count how many fish there are in a pond, you can't have 5.5 fish).

Probability distributions that generate discrete variables are discrete probability distributions.
